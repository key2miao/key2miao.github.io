<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Kai Wang</title>
  
  <meta name="author" content="Kai Wang">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/seal_icon.png">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Kai Wang</name>
              </p>
              <p>I am a MASc student and research assistant in <a href="https://www.concordia.ca/ginacody/electrical-computer-eng.html">Department of Eletrical Computer Engineering</a>, <a href="https://www.concordia.ca/">Concordia University</a>, advised by <a href="https://www.concordia.ca/faculty/wei-ping-zhu.html">Prof. Wei-Ping Zhu</a>. I also worked with <a href="https://scholar.google.com/citations?user=487syywAAAAJ&hl=en&oi=ao">Dr. Chao Xing</a>, <a href="https://scholar.google.com/citations?user=Q0hJ-hAAAAAJ&hl=en">Dr. Anderson Avila</a>, and <a href="https://scholar.google.com/citations?hl=en&user=MvXlF6kAAAAJ&view_op=list_works&sortby=pubdate">Dr. Mehdi Rezagholizadeh</a> as research intern in <a href="http://dev3.noahlab.com.hk/about.html">Huawei Noahâ€™s Ark Laboratory</a>, Montreal, Canada.
              </p>
              <p>
              I obtained my bachelor's degree in Electrical and Information Engineering from <a href="http://www.nuc.edu.cn/">North University of China</a>.
              </p>
              <p style="text-align:center">
                <a href="mailto:wangkai881617@gmail.com">Email</a> &nbsp/&nbsp
                <a href="data/Resume_Kai Wang.pdf">CV</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/kai-wang-3277951b4/">LinKedIn</a> &nbsp/&nbsp
                <a href="https://github.com/key2miao/">Github</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=dBVSZWAAAAAJ&hl=zh-CN">Google Scholar</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/KaiWang.jpg"><img style="width:80%;max-width:80%" alt="profile photo" src="images/KaiWang.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        <!-- research -->
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
              My research works focus on deep learning and its application to signal processing (speech, image and video), multi-modal learning, self-supervised learning. I am also quite interested in the applications of deep reinforcement learning such as communication network or social network.
              </p>
            </td>
          </tr>
            
        <!-- Publications -->
        
        
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

            </tbody></table>
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Publications</heading>
                </td>
              </tr>
            </tbody></table>
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

        <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='' width="200"></div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>CPTNN: Cross-Parallel Transformer Neural Network for Speech Enhancement in the Time Domain</papertitle>
              <br>
              <b> Kai Wang </b>,
              Bengbeng, He,
              Wei-Ping Zhu
              <br>
        <em>In submission to IEEE/ACM Transactions on Audio, Speech, and Language Processing (TASLP)</em>
              <br>
              <a href="">PDF</a>/
              <a href="">code</a>      Will release after acceptance!
              <!--<a href="https://zenodo.org/badge/latestdoi/250930736"><img src="https://zenodo.org/badge/250930736.svg" alt="DOI"></a> -->
              <p></p>
              <p>Proposed cross-parallel transformer neural network to extract and dynamically fuse the local and global information of long-range speech sequences.</p>
            </td>
          </tr>
        
        <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='' width="200"></div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>SE-Mixer: Towards An Efficient Attention-free Neural Network for Speech Enhancement</papertitle>
              <br>
              <b> Kai Wang </b>,
              Bengbeng, He,
              Wei-Ping Zhu
              <br>
        <em> In submission to IEEE Signal Processing Letters (SLP)</em>
              <br>
              <a href="">PDF</a>/
              <a href="">code</a>      Will release after acceptance!
              <!--<a href="https://zenodo.org/badge/latestdoi/250930736"><img src="https://zenodo.org/badge/250930736.svg" alt="DOI"></a> -->
              <p></p>
              <p>Proposed attention-free MLP architecture to achieve competitive performance compared with attention-based methods.</p>
            </td>
          </tr>
        
        <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='' width="200"></div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>SE-TransUNet: Transformers Make A Strong UNet for Time Domain Speech Enhancement</papertitle>
              <br>
              Bengbeng, He,
              <b> Kai Wang </b>,
              Wei-Ping Zhu
              <br>
        <em> In submission to European Signal Processing Conference (EURASIP)</em>, 2022
              <br>
              <a href="">PDF</a>/
              <a href="">code</a>      Will release after acceptance!
              <!--<a href="https://zenodo.org/badge/latestdoi/250930736"><img src="https://zenodo.org/badge/250930736.svg" alt="DOI"></a> -->
              <p></p>
              <p>Incorporate transformer structure into encoder and decoder of UNet to solve the limited receptive field of the CNN structure</p>
            </td>
          </tr>

        
          <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src='images/TSTB.jpg' width="200"></div>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                  <papertitle>TSTNN: Two-Stage Transformer Based Neural Network for Speech Enhancement in Time Domain</papertitle>
                <br>
                <b> Kai Wang </b>,
                Bengbeng, He,
                Wei-Ping Zhu
                <br>
          <em>IEEE International Conference on Acoustic, Speech and Signal Processing (ICASSP)</em>, 2021
                <br>
                <a href="https://arxiv.org/abs/2103.09963">arXiv</a>/
                <a href="https://github.com/key2miao/TSTNN">code</a>
                <!--<a href="https://zenodo.org/badge/latestdoi/250930736"><img src="https://zenodo.org/badge/250930736.svg" alt="DOI"></a> -->
                <p></p>
                <p>Proposed two-stage transformer neural network to extract local and global information of long-range speech feature.</p>
              </td>
            </tr>
          
          <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src='images/CAUNet.png' width="200"></div>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                  <papertitle>CAUNet: Context-Aware UNet for Speech Enhancement in Time Domain</papertitle>
                <br>
                <b> Kai Wang </b>,
                Bengbeng, He,
                Wei-Ping Zhu
                <br>
          <em>IEEE International Conference on Symposium on Circuits and Systems (ISCAS)</em>, 2021
                <br>
                <a href="https://ieeexplore.ieee.org/document/9401787">PDF</a>/
                <a href="https://github.com/key2miao/CAUNet">code</a>
                <!--<a href="https://zenodo.org/badge/latestdoi/250930736"><img src="https://zenodo.org/badge/250930736.svg" alt="DOI"></a> -->
                <p></p>
                <p>Proposed context-aware UNet to extract contextual information of speech feature.</p>
              </td>
            </tr>
          
      <!-- academic projects -->
      <!--
      </tbody></table>
      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <heading>Projects</heading>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          -->
      
        <!-- service -->
        
            
        
        <!-- research -->
       </tbody></table>
       <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
           <tr>
           <td style="padding:20px;width:100%;vertical-align:middle">
             <heading>Service</heading>
             <p>
             Conference reviewer: ISCAS 2021, NEWCAS 2021
             </p>
           </td>
         </tr>

       
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
          <td style="padding:0px">
            <br>
            <p style="text-align:right;font-size:small;">
            This website is inspired by <a href="https://jonbarron.info/">Jon Barron</a>. Thanks!
            </p>
            <p style="text-align:right;font-size:small;">
              (last update: Feb 2022)
              
            </p>
           
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
